================================================================================
READY TO RUN ON GOOGLE CLOUD!
================================================================================

Everything is prepared. Since you don't have gcloud CLI installed yet, here's
what you need to do:

================================================================================
OPTION 1: Install gcloud CLI First (RECOMMENDED)
================================================================================

1. **Install gcloud CLI:**
   Visit: https://cloud.google.com/sdk/docs/install
   
   For Linux (your system):
   ```bash
   cd ~
   curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-linux-x86_64.tar.gz
   tar -xf google-cloud-cli-linux-x86_64.tar.gz
   ./google-cloud-sdk/install.sh
   exec -l $SHELL
   gcloud init
   ```

2. **Then run our deployment script:**
   ```bash
   cd /u/arabel/cloud-computing/A8
   ./deploy_to_cloud.sh
   ```

That's it! The script will:
- Set your project
- Enable APIs
- Create cluster
- Submit job with the large data from gs://cs378n/TrainingData.txt
- Ask if you want to delete the cluster

================================================================================
OPTION 2: Use Google Cloud Console (NO CLI NEEDED)
================================================================================

If you prefer not to install gcloud CLI, use the web interface:

**Step 1: Go to Google Cloud Console**
https://console.cloud.google.com/dataproc

**Step 2: Select project cloud-computing-476020**

**Step 3: Create Cluster**
- Click "CREATE CLUSTER"
- Cluster name: spark-lr-cluster
- Region: us-central1
- Zone: us-central1-a  
- Cluster type: Standard
- Master node: n1-standard-4, 100GB disk
- Worker nodes: 4 x n1-standard-4, 100GB disk each
- Image version: 2.1 (Debian 11)
- Click CREATE

**Step 4: Upload Python Script**
- Go to Cloud Storage: https://console.cloud.google.com/storage
- Create a bucket (e.g., "yourname-cs378-bucket")
- Upload `logistic_regression_cloud.py` from /u/arabel/cloud-computing/A8/

**Step 5: Submit Job**
- Go back to Dataproc
- Click on your cluster
- Click "SUBMIT JOB"
- Job type: PySpark
- Main python file: gs://yourname-cs378-bucket/logistic_regression_cloud.py
- Arguments: gs://cs378n/TrainingData.txt
- Click SUBMIT

**Step 6: Monitor**
- Watch the job progress in the console
- Click on the job to see output logs

**Step 7: DELETE CLUSTER** (IMPORTANT!)
- After job completes, go to Clusters
- Select your cluster
- Click DELETE

================================================================================
OPTION 3: Manual gcloud Commands (After Installing CLI)
================================================================================

If you want more control, run these commands manually:

```bash
# Set project
gcloud config set project cloud-computing-476020

# Enable APIs  
gcloud services enable dataproc.googleapis.com
gcloud services enable compute.googleapis.com

# Create cluster (takes ~2-3 minutes)
gcloud dataproc clusters create spark-lr-cluster \
    --region=us-central1 \
    --zone=us-central1-a \
    --master-machine-type=n1-standard-4 \
    --num-workers=4 \
    --worker-machine-type=n1-standard-4 \
    --image-version=2.1-debian11

# Submit job (takes 30-60 minutes)
gcloud dataproc jobs submit pyspark \
    logistic_regression_cloud.py \
    --cluster=spark-lr-cluster \
    --region=us-central1 \
    --properties=spark.executor.memory=4g,spark.driver.memory=4g \
    -- gs://cs378n/TrainingData.txt

# DELETE cluster
gcloud dataproc clusters delete spark-lr-cluster \
    --region=us-central1 \
    --quiet
```

================================================================================
WHAT TO EXPECT
================================================================================

**Runtime:** 30-60 minutes for large dataset (~1.3M documents)
**Cost:** ~$2-5 total
**Output:** You'll see all 4 tasks complete with:
- Task 1: Full batch gradient descent results
- Task 2: Mini-batch GD with balanced sampling
- Task 3: MLlib LBFGS with 20K features
- Task 4: Confusion matrix and F1 score

**The output will show:**
- Training: ~1.04M docs (split 80/20 stratified)
- Test: ~260K docs
- Top 5 discriminative words from each task
- Final F1 score (should be meaningful with stratified split!)

================================================================================
FILES READY IN /u/arabel/cloud-computing/A8/
================================================================================

‚úÖ logistic_regression_cloud.py - The main script (updated with improvements)
‚úÖ deploy_to_cloud.sh - Automated deployment script  
‚úÖ CLOUD_SETUP.md - Detailed setup guide
‚úÖ QUICKSTART.md - Quick reference
‚úÖ SETUP_STEPS.md - Step-by-step instructions
‚úÖ README_CLOUD.txt - This file

================================================================================
WHICH OPTION SHOULD YOU CHOOSE?
================================================================================

**Best for beginners:** Option 2 (Web Console) - no CLI needed
**Best for automation:** Option 1 (deploy script) - one command
**Best for learning:** Option 3 (manual commands) - see each step

================================================================================
IMPORTANT REMINDERS
================================================================================

1. ‚ö†Ô∏è  **DELETE THE CLUSTER** after the job finishes to avoid charges!
2. üí∞ Cost is ~$2-5 for one complete run
3. ‚è±Ô∏è  Large dataset takes 30-60 minutes
4. üìä Data is already on Google Cloud (gs://cs378n/TrainingData.txt)
5. üîê You're using project: cloud-computing-476020

================================================================================
NEED HELP?
================================================================================

If you run into issues:
1. Check CLOUD_SETUP.md for troubleshooting
2. Verify project ID is correct: cloud-computing-476020
3. Make sure APIs are enabled
4. Check quotas in GCP Console

================================================================================

**Ready to deploy? Pick one of the 3 options above and let me know if you
need help with any step!**

================================================================================
