================================================================================
SIMPLEST CLOUD DEPLOYMENT - NO CLI NEEDED
================================================================================

Since authentication is required for gcloud, here's the EASIEST way to run
your code on Google Cloud using just your web browser:

================================================================================
STEP 1: Download Large Dataset (Optional - Only if Running Locally)
================================================================================

**NOTE:** If you're running on Google Cloud Dataproc, you DON'T need to download
the data - it's already available at gs://cs378n/TrainingData.txt

**Only download if you want to run locally or inspect the data:**

Training Dataset (Large): https://storage.googleapis.com/cs378n/TrainingData.txt
Testing Dataset (Large): https://storage.googleapis.com/cs378n/TestingData.txt

To download:
```bash
cd /u/arabel/cloud-computing/A8

# Download training data (~1.3M documents, ~400MB)
curl -O https://storage.googleapis.com/cs378n/TrainingData.txt

# Download testing data
curl -O https://storage.googleapis.com/cs378n/TestingData.txt
```

**For cloud execution, skip this step - the data is already on Google Cloud!**

================================================================================
STEP 2: Copy Your Python Script
================================================================================

Your script is ready at:
/u/arabel/cloud-computing/A8/logistic_regression_cloud.py

You'll need to upload this in Step 5.

================================================================================
STEP 3: Open Google Cloud Dataproc Console
================================================================================

Direct link:
https://console.cloud.google.com/dataproc/clusters?project=cloud-computing-476020

Click this link and login with your Google account.

================================================================================
STEP 4: Create Cluster (5 clicks)
================================================================================

On the Dataproc page:
1. Click "CREATE CLUSTER"
2. Click "CREATE" next to "Cluster on Compute Engine"
3. Give it a name: "my-spark-cluster"
4. Scroll down
5. Click "CREATE"

Wait 2-3 minutes for the cluster to start (green checkmark).

================================================================================
STEP 5: Upload and Run Your Script (6 clicks)
================================================================================

Once cluster shows green checkmark:
1. Click on "my-spark-cluster"
2. Click "SUBMIT JOB" at the top
3. Fill in:
   - Job type: PySpark
   - Main python file: Click "Browse" and upload logistic_regression_cloud.py
   - Arguments: gs://cs378n/TrainingData.txt
4. Click "SUBMIT"

The job will run for 30-60 minutes.

================================================================================
STEP 6: Check Results (2 clicks)
================================================================================

The job will take 30-60 minutes. You can:
- Watch the progress bar in the Jobs page
- Click on the job name to see real-time logs
- See all 4 tasks complete with results

Once complete, look at the output logs to see:
- Task 1: Full batch GD convergence
- Task 2: Mini-batch GD results  
- Task 3: MLlib training accuracy and top 5 words
- Task 4: Confusion matrix and F1 score

================================================================================
STEP 7: DELETE THE CLUSTER (CRITICAL!)
================================================================================

**DO NOT FORGET THIS STEP!**

1. Go to: https://console.cloud.google.com/dataproc/clusters?project=cloud-computing-476020
2. Check the box next to "spark-lr-cluster"
3. Click "DELETE" at the top
4. Confirm deletion

This prevents ongoing charges!

================================================================================
ALTERNATIVE: Even Simpler - Use Cloud Shell
================================================================================

If you want to use commands but avoid installing gcloud locally:

1. Go to: https://console.cloud.google.com
2. Click the Cloud Shell icon (>_) at the top right
3. In the Cloud Shell terminal, run these commands:

```bash
# Set project
gcloud config set project cloud-computing-476020

# Create cluster
gcloud dataproc clusters create spark-lr-cluster \
    --region=us-central1 \
    --master-machine-type=n1-standard-4 \
    --num-workers=4 \
    --worker-machine-type=n1-standard-4 \
    --image-version=2.1-debian11

# Upload your script first to a bucket, then submit:
# (Replace YOUR_BUCKET with your bucket name)
gcloud dataproc jobs submit pyspark \
    gs://YOUR_BUCKET/logistic_regression_cloud.py \
    --cluster=spark-lr-cluster \
    --region=us-central1 \
    -- gs://cs378n/TrainingData.txt

# Delete cluster when done
gcloud dataproc clusters delete spark-lr-cluster --region=us-central1 --quiet
```

================================================================================
SUMMARY
================================================================================

**Easiest Path:**
1. Open Google Cloud Console
2. Create Dataproc cluster (use settings above)
3. Upload Python script to Cloud Storage
4. Submit PySpark job with the script and data path
5. Wait for results
6. **DELETE CLUSTER**

**Time:** 5 minutes setup + 30-60 minutes run time
**Cost:** $2-5 total
**No CLI installation needed!**

================================================================================
